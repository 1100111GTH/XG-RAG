{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# 把父路径添加到检索目标中\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from config.config import llm_path\n",
    "######################################################\n",
    "from swift.llm import DatasetName, ModelType, SftArguments, sft_main, InferArguments, merge_lora_main, infer_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置需要使用的显卡设备号\n",
    "# nvidia-smi 查看显卡编号\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_args = SftArguments(\n",
    "    model_cache_dir=llm_path,\n",
    "    model_type=ModelType.qwen_72b_chat,\n",
    "    dataset='ms-bench',\n",
    "    train_dataset_sample=10000,\n",
    "    eval_steps=20,\n",
    "    logging_steps=5,\n",
    "    output_dir=\"output\",\n",
    "    lora_target_modules=\"ALL\",\n",
    "    self_cognition_sample=500,\n",
    "    model_name=[\"小光\", \"Xiao Guang\"],\n",
    "    model_author=[\"小光团队\", \"Team of Xiao Guang\"]\n",
    ")\n",
    "output = sft_main(sft_args)\n",
    "best_model_checkpoint = output[\"best_model_checkpoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_args = InferArguments(\n",
    "    model_cache_dir=llm_path,\n",
    "    # infer_backend=\"pt\", \n",
    "    # 设置需要使用的显卡数量，需要与环境变量 CUDA_VISIBLE_DEVICES 配合\n",
    "    # 与 swift deploy 不同，显卡数可以不是偶数\n",
    "    tensor_parallel_size=5, \n",
    "    # gpu_memory_utilization=0.72, \n",
    "    # 若使用了 checkpoint 则无需定义 model type\n",
    "    ckpt_dir=best_model_checkpoint,\n",
    "    eval_human=True,\n",
    "    merge_lora_and_save=True\n",
    ")\n",
    "\n",
    "# 开始推理测试（ 并执行微调合并 ）\n",
    "result = infer_main(infer_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
